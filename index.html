<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="VideoWorld: Exploring Knowledge Learning from Unlabeled Videos">
  <!-- <meta name="keywords" content="Nerfies, D-NeRF, NeRF"> -->
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>VideoWorld: Exploring Knowledge Learning from Unlabeled Videos</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/logo_v2.png">
  <link rel="stylesheet" href="./static/css/float.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <!-- <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.css"> -->
  <!-- <script src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.js"></script> -->
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
     
      "HTML-CSS": {
          styles: {
              '.MathJax_Display': {
                  color: "black"
              }
          }
      }
    });

    
  </script>
  
    <meta charset="UTF-8">
    <title>å›¾ç‰‡å±…ä¸­</title>
    <style>
        .center {
            text-align: center;
        }
    </style>


</head>
<body>

<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title georgia-font"><img src="./static/images/logo_v2.png" class="interpolation-image" alt="Interpolate start reference image." width="8%"/><font face="Georgia">VideoWorld: Exploring Knowledge Learning from Unlabeled Videos</font></h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=e5TJm-0AAAAJ&hl=zh-CN"><font face="Georgia">Zhongwei Ren</font></a><sup>1,3</sup>,</span>
            <span class="author-block">
              <a href="https://weiyc.github.io/"><font face="Georgia">Yunchao Wei</font></a><sup>1<math xmlns="http://www.w3.org/1998/Math/MathML"><mo>â€ </mo></math></sup>,
              <span class="author-block">
                <a href="https://openreview.net/profile?id=~Xun_Guo2"><font face="Georgia">Xun Guo</font></a><sup>2,3</sup>,</span>
            </span>
            <br>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=474TbQYAAAAJ&hl=zh-CN"><font face="Georgia">Yao Zhao</font></a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com.sg/citations?user=NmHgX-wAAAAJ&hl=en"><font face="Georgia">Bingyi Kang</font></a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://sites.google.com/site/jshfeng/home"><font face="Georgia">Jiashi Feng</font></a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com.sg/citations?user=OEZ816YAAAAJ&hl=en"><font face="Georgia">Xiaojie Jin</font></a><sup>3<math xmlns="http://www.w3.org/1998/Math/MathML"><mo>â€ </mo></math><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>â€¡</mo></math></sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup><font face="Georgia">Beijing Jiaotong University,</font></span>
            <span class="author-block"><sup>2</sup><font face="Georgia">University of Science and Technology of China,</font></span>
            <span class="author-block"><sup>3</sup><font face="Georgia">ByteDance Seed</font></span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">(<sup><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>â€ </mo></math></sup><font face="Georgia">Correspondence,</font><sup><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>â€¡</mo></math></sup><font face="Georgia">Project Lead)</font></span>
          </div>

         
          
          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2501.09781"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2501.09781"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://youtu.be/Cdi-qikbEzk"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/bytedance/VideoWorld"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span> -->
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
    <div class="column is-five-fifths">
    <div class="hero-body">
      <img src="./static/images/figure1.png"
                class="interpolation-image"
                alt="Interpolate start reference image."/>

                <div class="columns is-centered has-text-centered">
                  <div class="column is-five-fifths">
                    <div class="content has-text-justified">
                     
                      <p style="font-size: 20px;">
                        <font face="Georgia">Figure 1: VideoWorld explores learning knowledge from raw videos, ranging from task-specific rules to high-level reasoning and planning capabilities.Compared to other learning methods: reinforcement learning (RL), supervised learning (SL) and text-based learning, it offers three advantages: 1. better generalization with unified visual representation for various tasks and interfaces, 2. lower mannual annotation burden, and 3. richer real-world information than text description.</font>
                      </p>
                    </div>
                  </div>
                </div>
      </div>
    </div>
  </div>
    <div class="hero-body">
      <div class="center">
      <div class="container">
        <img src="./static/images/0.gif"  width="200" alt="Image 1">
        <img src="./static/images/01.gif" width="200" alt="Image 2">
        <img src="./static/images/02.gif" width="200" alt="Image 3">
        <img src="./static/images/05.gif" width="200" alt="Image 4">
      </div>
      <p>
        <font face="Georgia">Figure 2: VideoWorld plays Go by generating next board state.</font>
      </p>
      <br>
      <div class="center">
        <div class="container">
          <img src="./static/images/r0.gif"  width="200" alt="Image 1">
          <img src="./static/images/r1.gif" width="200" alt="Image 2">
          <img src="./static/images/r3.gif" width="200" alt="Image 3">
          <img src="./static/images/r4.gif" width="200" alt="Image 4">
        </div>
        <p>
          <font face="Georgia">Figure 3: VideoWorld controls robotic arms across different environments.</font>
        </p>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h2 class="title is-3"><img src="./static/images/logo_v2.png" class="interpolation-image" alt="Interpolate start reference image." width="6%"/>  <font face="Georgia">Abstract</font></h2>
        <div class="content has-text-justified">
          <p style="font-size: 20px;">
            <font face="Georgia">This work explores whether a deep generative model can learn complex knowledge solely from visual input, in contrast to the prevalent focus on text-based models like large language models (LLMs). We develop VideoWorld, an autoregressive video generation model trained on raw video data, and test its knowledge acquisition abilities in video-based Go and robotic control tasks. Our experiments reveal two key findings: (1) video-only training provides sufficient information for learning knowledge, including rules, reasoning and planning capabilities, and (2) increasing the compactness of visual representations significantly enhances learning efficiency. To improve both the efficiency and efficacy of knowledge learning, we introduce the Latent Dynamics Model (LDM) as a key component of VideoWorld. Remarkably, VideoWorld reaches a 5-dan professional level in the Video-GoBench with just a 300-million-parameter model, without relying on search algorithms or reward mechanisms typical in reinforcement learning. In robotic tasks, VideoWorld effectively learns diverse control operations and generalizes across environments, approaching the performance of oracle models in CALVIN and RLBench. This study opens new avenues for knowledge acquisition from visual data, with all code, data, and models to be open-sourced for further research.</font>
          </p>
        </div>
      </div>
    </div>
    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <!-- <h2 class="title is-3"></h2> -->
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/zNE-78wUD2c?si=s8R9LL9DwUBikY4A"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    <!--/ Paper video. -->
  </div>
</section>

<section class="hero is-light is-big", style="margin-top: -35px">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <br>
        <br>
        <h2 class="title is-3">ðŸ”¥<font face="Georgia">Highlights</font> </h2>
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-five-fifths">
        <font face="Georgia">
        <p style="font-size: 20px;">
          1. <b>We explore for the first time,</b> <span style="color: rgb(176, 152, 31)">if video generation models can learn sophisticated knowledge </span> and find that observing video alone can suffice to master complex tasks. Our model achieves superior training efficiency and demonstrates a 5-dan professional level of performance against the RL agent KataGO -- an impressive accomplishment, given that this level is challenging even for most human players and our model achieves this with only 300 million parameters and solely through visual observation, without using search or reward learning techniques typical in RL.
        </p>
        <br>
        <p style="font-size: 20px;">
          2. <b>We discover</b> <span style="color: rgb(176, 152, 31)">that compact representations of visual changes greatly boost the learning efficiency of reasoning capabilities,</span> and we also validate this in robotic scenarios. Based on this observation, e propose the Latent Dynamics Model (LDM), which enhances both the efficiency and effectiveness of video learning while providing
          a mechanism to probe the modelâ€™s learned representations and knowledge.
        </p>
        <br>
        <p style="font-size: 20px;">
          3. <b>We construct Video-GoBench</b> a large-scale Go video dataset, facilitating model training and evaluation for future research on video knowledge learning.
        </p>
      </font>
        <br><br>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <font face="Georgia">
  <div class="container is-max-desktop">

    <!-- <div class="columns is-centered"> -->

      <!-- Visual Effects. -->
      <!-- <div class="column">
        <div class="content">
          <h2 class="title is-3">Visual Effects</h2>
          <p>
            Using <i>nerfies</i> you can create fun visual effects. This Dolly zoom effect
            would be impossible without nerfies since it would require going through a wall.
          </p>
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/dollyzoom-stacked.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div> -->
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      <!-- <div class="column">
        <h2 class="title is-3">Matting</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              As a byproduct of our method, we can also solve the matting problem by ignoring
              samples that fall outside of a bounding box during rendering.
            </p>
            <video id="matting-video" controls playsinline height="100%">
              <source src="./static/videos/matting.mp4"
                      type="video/mp4">
            </video>
          </div>

        </div>
      </div>
    </div> -->
    <!--/ Matting. -->

    <!-- Animation. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3"> <img src="./static/images/logo_v2.png" class="interpolation-image" alt="Interpolate start reference image." width="5%"/>  Background</h2>
      </div>
    </div>
    <div class="columns is-centered ">
      <div class="column is-full-width">
        <div class="content has-text-justified">

        <!-- Interpolating. -->
        <!-- <h3 class="title is-4">Interpolating states</h3> -->
        <!-- <div class="columns is-vcentered interpolation-panel"> -->
          
          <p style="font-size: 20px;">
            The next token prediction training paradigm has endowed large language models (LLMs) with remarkable world knowledge and intelligence, enabling them to help address complex tasks that require reasoning, planning ahead, and decision-making. 
            However, language alone cannot fully capture all forms of knowledge or encompass the vast information present in the real world. In nature, biological organisms acquire knowledge primarily through visual information, rather than relying solely on language. For instance, gorillas and other primates learn vital skills like foraging and social interactions mainly through visual observation, mimicking adult behaviors without relying on language.
            </p>
            <p style="font-size: 20px;">
            Most existing research has focused on learning knowledge from texts, while relatively little attention is given to learning from pure visual signals. Some studies, such as UniPi, have explored using video data to train models for robot manipulation, but they still rely heavily on language instructions. Moreover, these tasks are often limited to single commands, without requiring complex reasoning or planning. This raises an important question: <b>can an AI model learn sophisticated knowledge solely from visual input, akin to how a gorilla learns from its environment? </b>

          </p>
        </div>
       </div>
    </div>

    <br>
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3"> <img src="./static/images/logo_v2.png" class="interpolation-image" alt="Interpolate start reference image." width="5%"/>  Our Work</h2>
      </div>
    </div>

    <div class="columns is-centered ">
      <div class="column is-full-width">
        <div class="content has-text-justified">

        <!-- Interpolating. -->
        <!-- <h3 class="title is-4">Interpolating states</h3> -->
        <!-- <div class="columns is-vcentered interpolation-panel"> -->
          
          <p style="font-size: 20px;">
            In this work, we take an initial step toward exploring  knowledge learning from raw video data by leveraging the next token prediction paradigm. To achieve this, we construct two experiment environments to collect pure visual training data: Go and robotic manipulation. 
            </p>
            <p style="font-size: 20px;">
              We begin our investigation with a basic video generation model based on VQ-VAE and autoregressive transformer. 
              The raw videos of task executions, collected from the  environments described above, serve as our  training data, representing the sole source  of knowledge. We  convert video frames into discrete tokens using VQ-VAE. Similar to large language models (LLMs), we train an autoregressive transformer   on these tokens, employing the next token (or next frame) prediction paradigm. During testing, the model generates   new frames based on prior frames, and   task-specific operations such as moves in Go or robotic operations are derived  from the newly generated frames. 
            
          </p>
          
          <h2 class="title is-4">Key findings</h2>
          <p style="font-size: 20px;">
            <b> 1. The model can learn basic knowledge from video generations.</b> This is evidenced by the its ability to  master Go rules and learn fundamental robotic operations.
            <!-- We define the video generation process for learning knowledge in the context of a specific task as a tuple \(\mathcal{G}=\left \langle \mathcal{X}, \mathcal{A}, \rho \right \rangle\), where \(\mathcal{X}\) is the observation space, \(\mathcal{A}\) is the action space, and \(\rho\) is a video generator. An offline dataset \(D=\{x_{1:T_n}^n\}_{n=1}^N\) consists of \(N\) video demonstrations within this environment, with \(T_n\) frames in each video sequence. -->
          </p>
          
          <p style="font-size: 20px;">
            <b> 2. The compactness of visual change representation is crucial for efficient knowledge learning.</b> The representation and abstraction of visual change are crucial for the model to effectively capture knowledge.
            <!-- Since the video frame sequences generated by \(\rho\) inherently contain the necessary information for task progression, we can directly learn a task-relevant mapping function  \(\pi\) that converts generated video frames into actions, Specifically,  \(\pi(\cdot|x_{1:t+1}):\mathcal{X}\rightarrow\mathcal{A}\), where \(\pi\) is a policy that maps the generated frames at time step \(t+1\) to appropriate action in \(\mathcal{A}\). This allows us to leverage the structure of generated videos to learn and execute the task without relying on explicit action annotations. -->
          </p>

          <h2 class="title is-4">Latent Dynamics Model</h2>
          <p style="font-size: 20px;">
            Building on the observations above, we propose the  Latent Dynamics Model (LDM). The LDM  compresses the future visual changes into a set of latent codes to serve as the compact representation of multi-step visual context. This allows the model to predict both video frames and latent code during training, improving its ability to capture and reason about diverse visual information, such as detailed object interactions and dynamic scene changes. In the figure below, the video generation model with LDM
            achieves superior training efficiency and demonstrates a 5-
            dan professional level of performance against the RL agent
          </p>
          
          <div class="center">
          <img src="./static/images/res_space_v3.png"
                class="interpolation-image"
                alt="Interpolate start reference image."  width="500"/>
                <figcaption style="font-size: 20px;"></figcaption>
              </div>

          <!-- <h2 class="title is-4">Basic Framework</h2>

          <p style="font-size: 20px;">
            We focus on using an auto-regressive video generator to instantiate \(\rho\). The basic framework includes a VQ-VAE encoder-decoder, and an auto-regressive transformer. The encoder converts video frames into discrete tokens, which the transformer uses for next-token prediction during training. During inference, the transformer generates discrete tokens for the next frame, which are subsequently converted back to pixel space by the decoder. 
          </p>
          <p style="font-size: 20px;">
            In our implementation, the VQ-VAE is instantiated using a custom MAGVIT-v2, equipped with an FSQ quantizer, while the transformer is implemented using the Llama architecture.  This setup allows us to generate high-quality frames, enabling effective modeling of complex visual sequences for reasoning tasks.
          </p> -->

          


        </div>
       </div>
    </div>
    <br>
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3"> <img src="./static/images/logo_v2.png" class="interpolation-image" alt="Interpolate start reference image." width="5%"/>  Overall Architecture</h2>
      </div>
    </div>

   
    <div class="columns is-centered ">
      <div class="column is-full-width">
        <div class="content has-text-justified">
          <p style="font-size: 20px;">
            <!-- While the basic framework can learn essential knowledge, its learning efficiency lags behind models trained in state space (i.e., move positions on the board). We attribute this to the less compact nature of the visual representation space (i.e., the output of encoder), motivating us to develop VideoWorld to learn within a compact latent visual representation space. -->
          </p>

          <img src="./static/images/overview_v6.png"
                class="interpolation-image"
                alt="Interpolate start reference image."/>
                <figcaption style="font-size: 20px;">Figure 4: Overview of the proposed VideoWorld model architecture. (Left) Overall architecture. (Right) The proposed latent dynamics model (LDM). First, LDM compresses the visual changes from each frame to its subsequent H frames into compact latent codes. Then, an auto-regressive transformer seamlessly integrates the output of LDM with the next token prediction paradigm.</figcaption>
        
          <p></p>
          
          
          
   <br>
   <div class="columns is-centered has-text-centered">
    <div class="column is-full-width">
      <h2 class="title is-3"> <img src="./static/images/logo_v2.png" class="interpolation-image" alt="Interpolate start reference image." width="5%"/>  Understanding Learned Knowledge with LDM</h2>
    </div>
  </div>
  <div class="columns is-centered ">
    <div class="column is-full-width">
      <div class="content has-text-justified">
        <p style="font-size: 20px;">
          The latent representation learned in LDM provides valuable insights into the knowledge learning process of VideoWorld. Below we offer an in-depth analysis of what our model learns through latent representations.
        </p>

        <h2 class="title is-4">LDM learns patterns in the training set</h2>
        <p style="font-size: 20px;">
          As shown in figure below, the latent codes on the training set capture both short- and long-term dependencies, demonstrating the model's ability to represent knowledge at different temporal scales. In the Go scenario, salient regions in the latent codes correspond to common move patterns, indicating that the model effectively embeds multi-step strategies into a compressed space, hence aiding decision-making and reasoning. Similarly, in the robotics scenario, the clustering of latent codes across steps reveals key dynamic dependencies over various time ranges, thus benefiting diverse manipulation tasks.
        </p>
        <img src="./static/images/umap_v4.png"
   class="interpolation-image"
   alt="Interpolate start reference image."/>
   <figcaption style="font-size: 20px;">Figure 5: UMAP projection of the learned latent code on the Go (Left) and CALVIN (right) training set. Each point represents
     the continuous (pre-quantization) latent code generated by the LDM. In Go examples, odd steps represent white's moves, and even steps represent black's moves. We visualize the latent codes of black moves in steps 2/4/6. The legend shows examples of common patterns learned for new black moves. For clarity, these moves are highlighted on the board with added colors and lines to indicate new patterns. On the right, we visualize the latent codes of the robotic arm's movement along the X/Y/Z axes at intervals of 1, 5, and 10 frames. Points are color-coded by displacement range, with purple and red indicating the maximum displacement in opposite directions along each axis.</figcaption>


        <h2 class="title is-4">LDM enables forward planning during testing</h2>
        <p style="font-size: 20px;">
          We examine the role of codes during inference. The visualization in the figure below shows that codes from different steps group by output positions, suggesting that VideoWorld models long-range changes progressively, similar to human forward-planning. The visualization also includes imagination of the opponent's moves, achieving a high average action-value of 71.2\% and action accuracy of 74.3\%. This indicates that, at each step, VideoWorld considers long-term changes in the game situation within the latent space, enabling it to make strategic moves with a long-term perspective.
        </p>
        <div class="center">
        <img src="./static/images/umap_test_v5.png"
                class="interpolation-image"
                alt="Interpolate start reference image." width="500"/>
                <figcaption style="font-size: 20px;">  </figcaption>
          
              </div>
              <p style="font-size: 20px;">
              Figure 6: Illustration of playing against KataGO and UMAP projection of the predicted latent code. Our model plays as black. The generated latent code is visualized through the LDM decoder and new stones in the visualization are marked with colors to match the legend. The visualization serves as a probe, indicating that the model shows signs of forward planning.
            </p>
            

          

              <h2 class="title is-4">LDM generates causally interrelated codes.</h2>
              <p style="font-size: 20px;">
                Similar findings are observed in the robotic scenario. We visualize the predicted latent codes during inference across different tasks in figure above. Here, \(H=9\), meaning the transformer generates 9 latent codes per time step, corresponding to 9 prediction steps. As shown, the latent codes for different prediction steps are grouped by task type, indicating that they capture task-relevant dynamics. Codes for steps 1â€“4 show greater overlap, likely because they focus on fine-grained displacements shared across tasks. In contrast, steps 5â€“9 show more distinct separation by task type, highlighting the model's ability to progressively capture long-range changes specific to each task.
              </p>

        <div class="center">
          <img src="./static/images/umap_calvin.png"
                  class="interpolation-image"
                  alt="Interpolate start reference image." width="500"/>
                  <figcaption style="font-size: 20px;">  </figcaption>
            
                </div>
                <p style="font-size: 20px;">
                  Figure 7: Illustration of robotic manipulation and UMAP projection of the predicted latent code during inference. Latent codes are visualized through the LDM decoder. The UMAP projection illustrates the 9 predicted latent codes (i.e. \(H=9\)) across different tasks, with each point color-coded by task type. Visualizations with a yellow background show the model's actual robotic arm control during inference, while those with a green background represent the model's next-frame predictions during training.
                </p>
        
      </div>
    </div>
 </div>
        <!-- </div>
        <div class="column is-centered has-text-centered">
          <img src="./static/images/overallV3.png"
                class="interpolation-image"
                alt="Interpolate start reference image."/>
                <figcaption style="font-size: 20px;">Overview of the proposed PixelLM model architecture. (Left) Overall architecture. (Right) The proposed lightweigh pixel decoder. Trainable LoRA parameters are incorporated into the LLM. All parameters except those for the CLIP encoder and LLM are trainable.</figcaption>
        </div> -->
        
        <br/>
        
            
       

         
        </div>

      </div>
    </div>


  </div>
</font>
</section>


<section class="section" id="BibTeX">
  <font face="Georgia">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{ren2025videoworldexploringknowledgelearning,
  title={VideoWorld: Exploring Knowledge Learning from Unlabeled Videos}, 
  author={Zhongwei Ren and Yunchao Wei and Xun Guo and Yao Zhao and Bingyi Kang and Jiashi Feng and Xiaojie Jin},
  year={2025},
  eprint={2501.09781},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  url={https://arxiv.org/abs/2501.09781}, 
}
</code></pre>
  </div>
</font>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This website is adapted from <a
            href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, licensed under a <a rel="license"
            href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
Commons Attribution-ShareAlike 4.0 International License</a>. 
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
