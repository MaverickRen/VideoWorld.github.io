<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="VideoWorld: Exploring Knowledge Learning from Unlabeled Videos">
  <!-- <meta name="keywords" content="Nerfies, D-NeRF, NeRF"> -->
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>VideoWorld: Exploring Knowledge Learning from Unlabeled Videos</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/logo_v2.png">
  <link rel="stylesheet" href="./static/css/float.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <!-- <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.css"> -->
  <!-- <script src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.js"></script> -->
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
     
      "HTML-CSS": {
          styles: {
              '.MathJax_Display': {
                  color: "black"
              }
          }
      }
    });

    
  </script>
  
    <meta charset="UTF-8">
    <title>图片居中</title>
    <style>
        .center {
            text-align: center;
        }
    </style>


</head>
<body>

<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title"><img src="./static/images/logo_v2.png" class="interpolation-image" alt="Interpolate start reference image." width="6%"/> VideoWorld: Exploring Knowledge Learning from Unlabeled Videos</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=e5TJm-0AAAAJ&hl=zh-CN">Zhongwei Ren</a><sup>1,3</sup>,</span>
            <span class="author-block">
              <a href="https://weiyc.github.io/">Yunchao Wei</a><sup>1<math xmlns="http://www.w3.org/1998/Math/MathML"><mo>†</mo></math></sup>,
              <span class="author-block">
                <a href="https://openreview.net/profile?id=~Xun_Guo2">Xun Guo</a><sup>2,3</sup>,</span>
            </span>
            <br>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=474TbQYAAAAJ&hl=zh-CN">Yao Zhao</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com.sg/citations?user=NmHgX-wAAAAJ&hl=en">Bingyi Kang</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://sites.google.com/site/jshfeng/home">Jiashi Feng</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com.sg/citations?user=OEZ816YAAAAJ&hl=en">Xiaojie Jin</a><sup>3<math xmlns="http://www.w3.org/1998/Math/MathML"><mo>†</mo></math><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>‡</mo></math></sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Beijing Jiaotong University,</span>
            <span class="author-block"><sup>2</sup>University of Science and Technology of China,</span>
            <span class="author-block"><sup>3</sup>ByteDance Seed</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">(<sup><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>†</mo></math></sup>Correspondence,<sup><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>‡</mo></math></sup>Project Lead)</span>
          </div>

         
          
          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://youtu.be/Cdi-qikbEzk"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/bytedance/VideoWorld"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span> -->
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/figure1_showv6.png"
                class="interpolation-image"
                alt="Interpolate start reference image."/>
      <h2 class="subtitle has-text-centered">
        VideoWorld explores learning knowledge from raw videos, ranging from task-specific rules to high-level reasoning and planning capabilities.Compared to other learning methods: reinforcement learning (RL), supervised learning (SL) and text-based learning, it offers three advantages: 1. better generalization with unified visual representation for various tasks and interfaces, 2. lower mannual annotation burden, and 3. richer real-world information than text description.
      </h2>

      <div class="center">
      <div class="container">
        <img src="./static/images/0.gif"  width="200" alt="Image 1">
        <img src="./static/images/01.gif" width="200" alt="Image 2">
        <img src="./static/images/02.gif" width="200" alt="Image 3">
        <img src="./static/images/05.gif" width="200" alt="Image 4">
      </div>
      <p>
        VideoWorld plays Go by generating next board state.
      </p>

      <div class="center">
        <div class="container">
          <img src="./static/images/r0.gif"  width="200" alt="Image 1">
          <img src="./static/images/r1.gif" width="200" alt="Image 2">
          <img src="./static/images/r3.gif" width="200" alt="Image 3">
          <img src="./static/images/r4.gif" width="200" alt="Image 4">
        </div>
        <p>
          VideoWorld controls robotic arms across different environments.
        </p>

    </div>

    </div>
  </div>
</section>

<!-- 
<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/steve.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/shiba.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/mask.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/coffee.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->



<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h2 class="title is-3"><img src="./static/images/logo_v2.png" class="interpolation-image" alt="Interpolate start reference image." width="6%"/>  Abstract</h2>
        <div class="content has-text-justified">
          <!-- <p>
            We present the first method capable of photorealistically reconstructing a non-rigidly
            deforming scene using photos/videos captured casually from mobile phones.
          </p>
          <p>
            Our approach augments neural radiance fields
            (NeRF) by optimizing an
            additional continuous volumetric deformation field that warps each observed point into a
            canonical 5D NeRF.
            We observe that these NeRF-like deformation fields are prone to local minima, and
            propose a coarse-to-fine optimization method for coordinate-based models that allows for
            more robust optimization.
            By adapting principles from geometry processing and physical simulation to NeRF-like
            models, we propose an elastic regularization of the deformation field that further
            improves robustness.
          </p> -->
          <p style="font-size: 20px;">
            This work explores whether a deep generative model can learn complex knowledge solely from visual input, in contrast to the prevalent focus on text-based models like large language models (LLMs). We develop VideoWorld, an autoregressive video generation model trained on raw video data, and test its knowledge acquisition abilities in video-based Go and robotic control tasks. Our experiments reveal two key findings: (1) video-only training provides sufficient information for learning knowledge, including rules, reasoning and planning capabilities, and (2) increasing the compactness of visual representations significantly enhances learning efficiency. To improve both the efficiency and efficacy of knowledge learning, we introduce the Latent Dynamics Model (LDM) as a key component of VideoWorld. Remarkably, VideoWorld reaches a 5-dan professional level in the Video-GoBench with just a 300-million-parameter model, without relying on search algorithms or reward mechanisms typical in reinforcement learning. In robotic tasks, VideoWorld effectively learns diverse control operations and generalizes across environments, approaching the performance of oracle models in CALVIN and RLBench. This study opens new avenues for knowledge acquisition from visual data, with all code, data, and models to be open-sourced for further research.
          </p>
        </div>
      </div>
    </div>
    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <!-- <h2 class="title is-3"></h2> -->
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/zNE-78wUD2c?si=s8R9LL9DwUBikY4A"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    <!--/ Paper video. -->
  </div>
</section>

<section class="hero is-light is-big", style="margin-top: -35px">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <br>
        <br>
        <h2 class="title is-3">🔥 Highlights</h2>
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-five-fifths">
        <p style="font-size: 20px;">
          1. <b>We explore for the first time,</b> <span style="color: rgb(176, 152, 31)">if video generation models can learn sophisticated knowledge </span> and find that observing video alone can suffice to master complex tasks. Our model achieves superior training efficiency and demonstrates a 5-dan professional level of performance against the RL agent KataGO -- an impressive accomplishment, given that this level is challenging even for most human players and our model achieves this with only 300 million parameters and solely through visual observation, without using search or reward learning techniques typical in RL.
        </p>
        <br>
        <p style="font-size: 20px;">
          2. <b>We discover</b> <span style="color: rgb(176, 152, 31)">that compact representations of visual changes greatly boost the learning efficiency of reasoning capabilities,</span> and we also validate this in robotic scenarios. Based on this observation, e propose the Latent Dynamics Model (LDM), which enhances both the efficiency and effectiveness of video learning while providing
          a mechanism to probe the model’s learned representations and knowledge.
        </p>
        <br>
        <p style="font-size: 20px;">
          3. <b>We construct Video-GoBench</b> a large-scale Go video dataset, facilitating model training and evaluation for future research on video knowledge learning.
        </p>
        <br><br>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <!-- <div class="columns is-centered"> -->

      <!-- Visual Effects. -->
      <!-- <div class="column">
        <div class="content">
          <h2 class="title is-3">Visual Effects</h2>
          <p>
            Using <i>nerfies</i> you can create fun visual effects. This Dolly zoom effect
            would be impossible without nerfies since it would require going through a wall.
          </p>
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/dollyzoom-stacked.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div> -->
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      <!-- <div class="column">
        <h2 class="title is-3">Matting</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              As a byproduct of our method, we can also solve the matting problem by ignoring
              samples that fall outside of a bounding box during rendering.
            </p>
            <video id="matting-video" controls playsinline height="100%">
              <source src="./static/videos/matting.mp4"
                      type="video/mp4">
            </video>
          </div>

        </div>
      </div>
    </div> -->
    <!--/ Matting. -->

    <!-- Animation. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3"> <img src="./static/images/logo_v2.png" class="interpolation-image" alt="Interpolate start reference image." width="5%"/>  Background</h2>
      </div>
    </div>
    <div class="columns is-centered ">
      <div class="column is-full-width">
        <div class="content has-text-justified">

        <!-- Interpolating. -->
        <!-- <h3 class="title is-4">Interpolating states</h3> -->
        <!-- <div class="columns is-vcentered interpolation-panel"> -->
          
          <p style="font-size: 20px;">
            The next token prediction training paradigm has endowed large language models (LLMs) with remarkable world knowledge and intelligence, enabling them to help address complex tasks that require reasoning, planning ahead, and decision-making. 
            However, language alone cannot fully capture all forms of knowledge or encompass the vast information present in the real world. In nature, biological organisms acquire knowledge primarily through visual information, rather than relying solely on language. For instance, gorillas and other primates learn vital skills like foraging and social interactions mainly through visual observation, mimicking adult behaviors without relying on language.
            </p>
            <p style="font-size: 20px;">
            Most existing research has focused on learning knowledge from texts, while relatively little attention is given to learning from pure visual signals. Some studies, such as UniPi, have explored using video data to train models for robot manipulation, but they still rely heavily on language instructions. Moreover, these tasks are often limited to single commands, without requiring complex reasoning or planning. This raises an important question: <b>can an AI model learn sophisticated knowledge solely from visual input, akin to how a gorilla learns from its environment? </b>

          </p>
        </div>
       </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3"> <img src="./static/images/logo_v2.png" class="interpolation-image" alt="Interpolate start reference image." width="5%"/>  Our Work</h2>
      </div>
    </div>

    <div class="columns is-centered ">
      <div class="column is-full-width">
        <div class="content has-text-justified">

        <!-- Interpolating. -->
        <!-- <h3 class="title is-4">Interpolating states</h3> -->
        <!-- <div class="columns is-vcentered interpolation-panel"> -->
          
          <p style="font-size: 20px;">
            In this work, we take an initial step toward exploring  knowledge learning from raw video data by leveraging the next token prediction paradigm. To achieve this, we construct two experiment environments to collect pure visual training data: Go and robotic manipulation. 
            </p>
            <p style="font-size: 20px;">
              We begin our investigation with a basic video generation model based on VQ-VAE and autoregressive transformer. 
              The raw videos of task executions, collected from the  environments described above, serve as our  training data, representing the sole source  of knowledge. We  convert video frames into discrete tokens using VQ-VAE. Similar to large language models (LLMs), we train an autoregressive transformer   on these tokens, employing the next token (or next frame) prediction paradigm. During testing, the model generates   new frames based on prior frames, and   task-specific operations such as moves in Go or robotic operations are derived  from the newly generated frames. 
            
          </p>
          
          <h2 class="title is-4">Key findings</h2>
          <p style="font-size: 20px;">
            <b> 1. The model can learn basic knowledge from video generations.</b> This is evidenced by the its ability to  master Go rules and learn fundamental robotic operations.
            <!-- We define the video generation process for learning knowledge in the context of a specific task as a tuple \(\mathcal{G}=\left \langle \mathcal{X}, \mathcal{A}, \rho \right \rangle\), where \(\mathcal{X}\) is the observation space, \(\mathcal{A}\) is the action space, and \(\rho\) is a video generator. An offline dataset \(D=\{x_{1:T_n}^n\}_{n=1}^N\) consists of \(N\) video demonstrations within this environment, with \(T_n\) frames in each video sequence. -->
          </p>
          
          <p style="font-size: 20px;">
            <b> 2. The compactness of visual change representation is crucial for efficient knowledge learning.</b> The representation and abstraction of visual change are crucial for the model to effectively capture knowledge.
            <!-- Since the video frame sequences generated by \(\rho\) inherently contain the necessary information for task progression, we can directly learn a task-relevant mapping function  \(\pi\) that converts generated video frames into actions, Specifically,  \(\pi(\cdot|x_{1:t+1}):\mathcal{X}\rightarrow\mathcal{A}\), where \(\pi\) is a policy that maps the generated frames at time step \(t+1\) to appropriate action in \(\mathcal{A}\). This allows us to leverage the structure of generated videos to learn and execute the task without relying on explicit action annotations. -->
          </p>

          <h2 class="title is-4">Latent Dynamics Model</h2>
          <p style="font-size: 20px;">
            Building on the observations above, we propose the  Latent Dynamics Model (LDM). The LDM  compresses the future visual changes into a set of latent codes to serve as the compact representation of multi-step visual context. This allows the model to predict both video frames and latent code during training, improving its ability to capture and reason about diverse visual information, such as detailed object interactions and dynamic scene changes. In the figure below, the video generation model with LDM
            achieves superior training efficiency and demonstrates a 5-
            dan professional level of performance against the RL agent
          </p>
          
          <div class="center">
          <img src="./static/images/rep_space_v1.png"
                class="interpolation-image"
                alt="Interpolate start reference image."  width="500"/>
                <figcaption style="font-size: 20px;"></figcaption>
              </div>

          <!-- <h2 class="title is-4">Basic Framework</h2>

          <p style="font-size: 20px;">
            We focus on using an auto-regressive video generator to instantiate \(\rho\). The basic framework includes a VQ-VAE encoder-decoder, and an auto-regressive transformer. The encoder converts video frames into discrete tokens, which the transformer uses for next-token prediction during training. During inference, the transformer generates discrete tokens for the next frame, which are subsequently converted back to pixel space by the decoder. 
          </p>
          <p style="font-size: 20px;">
            In our implementation, the VQ-VAE is instantiated using a custom MAGVIT-v2, equipped with an FSQ quantizer, while the transformer is implemented using the Llama architecture.  This setup allows us to generate high-quality frames, enabling effective modeling of complex visual sequences for reasoning tasks.
          </p> -->

          


        </div>
       </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3"> <img src="./static/images/logo_v2.png" class="interpolation-image" alt="Interpolate start reference image." width="5%"/>  Overall Architecture</h2>
      </div>
    </div>

    <div class="columns is-centered ">
      <div class="column is-full-width">
        <div class="content has-text-justified">
          <p style="font-size: 20px;">
            <!-- While the basic framework can learn essential knowledge, its learning efficiency lags behind models trained in state space (i.e., move positions on the board). We attribute this to the less compact nature of the visual representation space (i.e., the output of encoder), motivating us to develop VideoWorld to learn within a compact latent visual representation space. -->
          </p>

          <img src="./static/images/overview_v6.png"
                class="interpolation-image"
                alt="Interpolate start reference image."/>
                <figcaption style="font-size: 20px;">Overview of the proposed VideoWorld model architecture. (Left) Overall architecture. (Right) The proposed latent dynamics model (LDM). First, LDM compresses the visual changes from each frame to its subsequent H frames into compact latent codes. Then, an auto-regressive transformer seamlessly integrates the output of LDM with the next token prediction paradigm.</figcaption>
        
          <p></p>
          
          
          <!-- <h2 class="title is-4">Latent Dynamics Model</h2>
          <p style="font-size: 20px;">
            Video encoding typically requires hundreds or thousands of VQ tokens to capture the full range of visual information, leading to sparse embedding of knowledge across these tokens. To enhance efficiency, we introduce a latent dynamics model that uses query embeddings to represent visual changes across multiple frames, specifically for reasoning and planning tasks. For example,   multi-step board changes in Go or continuous actions in robotics exhibit strong temporal correlations. By compressing these multi-step changes into compact embeddings, we not only increase the compactness of policy information but also encode guidance for forward planning.
          </p>

          <p style="font-size: 20px;">
            The model employs a MAGVITv2-style causal encoder-decoder, while intentionally omitting temporal downsampling to preserve detail in each frame. 
 For a video clip \(x_{1:T}\), we sample each frame \(x_t\) along with the subsequent \(H\) frames, denoted as \(x_{t+1:t+H}\). If fewer than \(H\) steps remain, we apply replication padding.
In the figure above, the encoder first extracts visual feature maps \(f_{t:t+H}\) in a causal manner. Importantly, these features are not quantized, allowing them to retain detailed temporal information. 
          </p>

          <p style="font-size: 20px;">
            Next, we define a set of attention blocks and corresponding learnable embeddings
\(\{q^h\}_{h=1}^H\). Each query \(q^h\), via the attention mechanism, captures change information in \(f_{t:t+h}\), yielding a continuous latent representation   \(\tilde{z}_t^h\), which is then quantized with a discrete codebook by FSQ. This quantized representation serves as an information bottleneck, preventing the LDM from learning shortcuts (e.g. trivially copying \(f_{t+h}\) to \(z_t^h\)). 
          </p>

          <p style="font-size: 20px;">
            Finally, the decoder uses the feature map \(f_t\) and the latent change embeddings \(\{z_t^h\}_{h=1}^H\) to predict the subsequent frames \(\hat{x}_{t+1:t+H}\) in a causal manner. The training objective of LDM is minimize the \(\ell_2\) distance between \(x_{t+h}\) and \(\hat{x}_{t+h}\).
          </p>

          <p style="font-size: 20px;">
            By sequentially encoding changes from \(x_t\) to \(x_{t+H}\) using multiple embeddings, we achieve a compact and efficient representation of temporal dynamics, which is crucial for long-term reasoning and planning tasks. As shown in the figure below, this model can learn meaningful embeddings that capture both short- and long-term dependencies in visual sequences.  
          </p>

          <img src="./static/images/umap_v4.png"
                class="interpolation-image"
                alt="Interpolate start reference image."/>
                <figcaption style="font-size: 20px;">UMAP projection of the learned latent code on the Go (Left) and CALVIN (right) training set. Each point represents
                  the continuous (pre-quantization) latent code generated by the LDM. In Go examples, odd steps represent white's moves, and even steps represent black's moves. We visualize the latent codes of black moves in steps 2/4/6. The legend shows examples of common patterns learned for new black moves. For clarity, these moves are highlighted on the board with added colors and lines to indicate new patterns. On the right, we visualize the latent codes of the robotic arm's movement along the X/Y/Z axes at intervals of 1, 5, and 10 frames. Points are color-coded by displacement range, with purple and red indicating the maximum displacement in opposite directions along each axis.</figcaption>
          
          <h2 class="title is-4">Autoregressive Transformer</h2>
          <p style="font-size: 20px;">
            The auto-regressive transformer is employed as the video generator,  and it seamlessly integrates the output of the LDM with the next token prediction paradigm. This integration offers several benefits, including improved modeling of long-term dependencies, enhanced reasoning capabilities, and more efficient learning of complex visual patterns over time. 
          </p>

          <p style="font-size: 20px;">
            For each video \(x_{1:T}\),  we first get latent codes \(\{z_t^h\}_{t=1,h=1}^{T,H}\) using the latent dynamics model. These latent codes, along with discretized video frames are combined into a sequence for autoregressive prediction. 
The codebook used by the video decoder is distinct from the one used in the latent dynamics model, and the vocabulary of the auto-regressive transformer is the union of both. This allows the transformer to leverage both the fine-grained visual details captured by the video encoder-decoder and the compact, task-relevant embeddings produced by LDM, enabling it to generate both visually coherent frames and maintain the underlying temporal dynamics captured by the LDM.
          </p>

          <h2 class="title is-4">Mapping to Task Operation</h2>
          <p style="font-size: 20px;">
            During inference, at each time step \(t\), we use the transformer to auto-regressively generate the latent codes  \(\{\hat{z}_t^h\}_{h=1}^{H}\) and the predicted frame \(\hat{x}_{t+1}\). To convert the generated content into actionable decisions for specific tasks, we further train an Inverse Dynamics Model (IDM) \(\pi\) like~\cite{du2023unipi}.  IDM consists of several MLP layers and is trained independently from the video generator, using a small amount of video action label data. 
          </p>

          <p style="font-size: 20px;">
            In the basic framework, the IDM takes the current frame   \(x_t\)  and the predicted frame  \(\hat{x}_{t+1}\) to generate the corresponding action: \(\pi(\cdot|x_t, \hat{x}_{t+1})\). When incorporating the LDM, the IDM is extended to take both the predicted frame and the latent codes, resulting in: \(\pi(\cdot|x_t, \hat{x}_{t+1}, \{\hat{z}_t^h\}_{h=1}^{H})\). This allows the IDM to leverage the rich temporal representations encoded by LDM, enhancing its ability to produce more informed and contextually accurate actions.
          </p>


        </div>
      </div>
   </div> -->
   
   <div class="columns is-centered has-text-centered">
    <div class="column is-full-width">
      <h2 class="title is-3"> <img src="./static/images/logo_v2.png" class="interpolation-image" alt="Interpolate start reference image." width="5%"/>  Understanding Learned Knowledge with LDM</h2>
    </div>
  </div>
  <div class="columns is-centered ">
    <div class="column is-full-width">
      <div class="content has-text-justified">
        <p style="font-size: 20px;">
          The latent representation learned in LDM provides valuable insights into the knowledge learning process of VideoWorld. Below we offer an in-depth analysis of what our model learns through latent representations.
        </p>

        <h2 class="title is-4">LDM learns patterns in the training set</h2>
        <p style="font-size: 20px;">
          As shown in figure below, the latent codes on the training set capture both short- and long-term dependencies, demonstrating the model's ability to represent knowledge at different temporal scales. In the Go scenario, salient regions in the latent codes correspond to common move patterns, indicating that the model effectively embeds multi-step strategies into a compressed space, hence aiding decision-making and reasoning. Similarly, in the robotics scenario, the clustering of latent codes across steps reveals key dynamic dependencies over various time ranges, thus benefiting diverse manipulation tasks.
        </p>
        <img src="./static/images/umap_v4.png"
   class="interpolation-image"
   alt="Interpolate start reference image."/>
   <figcaption style="font-size: 20px;">UMAP projection of the learned latent code on the Go (Left) and CALVIN (right) training set. Each point represents
     the continuous (pre-quantization) latent code generated by the LDM. In Go examples, odd steps represent white's moves, and even steps represent black's moves. We visualize the latent codes of black moves in steps 2/4/6. The legend shows examples of common patterns learned for new black moves. For clarity, these moves are highlighted on the board with added colors and lines to indicate new patterns. On the right, we visualize the latent codes of the robotic arm's movement along the X/Y/Z axes at intervals of 1, 5, and 10 frames. Points are color-coded by displacement range, with purple and red indicating the maximum displacement in opposite directions along each axis.</figcaption>


        <h2 class="title is-4">LDM enables forward planning during testing</h2>
        <p style="font-size: 20px;">
          We examine the role of codes during inference. The visualization in the figure below shows that codes from different steps group by output positions, suggesting that VideoWorld models long-range changes progressively, similar to human forward-planning. The visualization also includes imagination of the opponent's moves, achieving a high average action-value of 71.2\% and action accuracy of 74.3\%. This indicates that, at each step, VideoWorld considers long-term changes in the game situation within the latent space, enabling it to make strategic moves with a long-term perspective.
        </p>
        <div class="center">
        <img src="./static/images/umap_test_v5.png"
                class="interpolation-image"
                alt="Interpolate start reference image." width="500"/>
                <figcaption style="font-size: 20px;">  </figcaption>
          
              </div>
              <p style="font-size: 20px;">
              Illustration of playing against KataGO and UMAP projection of the predicted latent code. Our model plays as black. The generated latent code is visualized through the LDM decoder and new stones in the visualization are marked with colors to match the legend. The visualization serves as a probe, indicating that the model shows signs of forward planning.
            </p>
            

          

              <h2 class="title is-4">LDM generates causally interrelated codes.</h2>
              <p style="font-size: 20px;">
                Similar findings are observed in the robotic scenario. We visualize the predicted latent codes during inference across different tasks in figure above. Here, \(H=9\), meaning the transformer generates 9 latent codes per time step, corresponding to 9 prediction steps. As shown, the latent codes for different prediction steps are grouped by task type, indicating that they capture task-relevant dynamics. Codes for steps 1–4 show greater overlap, likely because they focus on fine-grained displacements shared across tasks. In contrast, steps 5–9 show more distinct separation by task type, highlighting the model's ability to progressively capture long-range changes specific to each task.
              </p>

        <div class="center">
          <img src="./static/images/umap_calvin.png"
                  class="interpolation-image"
                  alt="Interpolate start reference image." width="500"/>
                  <figcaption style="font-size: 20px;">  </figcaption>
            
                </div>
                <p style="font-size: 20px;">
                  Illustration of robotic manipulation and UMAP projection of the predicted latent code during inference. Latent codes are visualized through the LDM decoder. The UMAP projection illustrates the 9 predicted latent codes (i.e. \(H=9\)) across different tasks, with each point color-coded by task type. Visualizations with a yellow background show the model's actual robotic arm control during inference, while those with a green background represent the model's next-frame predictions during training.
                </p>
        
      </div>
    </div>
 </div>
        <!-- </div>
        <div class="column is-centered has-text-centered">
          <img src="./static/images/overallV3.png"
                class="interpolation-image"
                alt="Interpolate start reference image."/>
                <figcaption style="font-size: 20px;">Overview of the proposed PixelLM model architecture. (Left) Overall architecture. (Right) The proposed lightweigh pixel decoder. Trainable LoRA parameters are incorporated into the LLM. All parameters except those for the CLIP encoder and LLM are trainable.</figcaption>
        </div> -->
        
        <br/>
        
            
       

         
        </div>

      </div>
    </div>


  </div>




<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
      @misc{ren2025videoworldexploringknowledgelearning,
        title={VideoWorld: Exploring Knowledge Learning from Unlabeled Videos}, 
        author={Zhongwei Ren and Yunchao Wei and Xun Guo and Yao Zhao and Bingyi Kang and Jiashi Feng and Xiaojie Jin},
        year={2025},
        eprint={2501.09781},
        archivePrefix={arXiv},
        primaryClass={cs.CV},
        url={https://arxiv.org/abs/2501.09781}, 
  }
</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This website is adapted from <a
            href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, licensed under a <a rel="license"
            href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
Commons Attribution-ShareAlike 4.0 International License</a>. 
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
